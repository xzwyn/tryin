{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":345400,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":288607,"modelId":309370}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"3e1efaf3-f9a0-4484-b266-6e01ef69bcf5","_cell_guid":"9a62b570-7e41-4560-ad1f-3fd7717a5a7c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-25T16:02:23.385756Z","iopub.execute_input":"2025-02-25T16:02:23.386019Z","iopub.status.idle":"2025-02-25T16:02:23.675959Z","shell.execute_reply.started":"2025-02-25T16:02:23.385998Z","shell.execute_reply":"2025-02-25T16:02:23.675367Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nAPI_KEY = user_secrets.get_secret(\"API_KEY\")\n\nprint(\"done\")","metadata":{"_uuid":"4d995c59-2d34-45ce-84d4-ccdb32e75867","_cell_guid":"619eac86-e3ca-47ca-95ce-057c6bdbf7d6","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-02-28T18:11:23.022894Z","iopub.execute_input":"2025-02-28T18:11:23.023201Z","iopub.status.idle":"2025-02-28T18:11:23.159855Z","shell.execute_reply.started":"2025-02-28T18:11:23.023174Z","shell.execute_reply":"2025-02-28T18:11:23.159125Z"}},"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import requests \nimport json\n\nBASE_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\ninput_text = \"a field of blue flowers\"\n\ndata = {\n    \"model\": \"deepseek/deepseek-chat:free\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": f\"Describe {input_text} in vivid detail, focusing on its appearance, movement, and sensory qualities. Use rich, descriptive language to bring the scene to life, emphasizing colors, textures, and any dynamic changes over time. Imagine the process or moment as if it were unfolding before your eyes, and convey its beauty or significance. Limit the number of words to 50\"\n        }\n    ]\n}","metadata":{"_uuid":"4c1a6805-c71c-4e7f-8348-5b6919cfc018","_cell_guid":"aed0a8c7-f59f-4eed-8ea4-5c8816a5de9b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-28T18:11:46.676527Z","iopub.execute_input":"2025-02-28T18:11:46.676846Z","iopub.status.idle":"2025-02-28T18:11:46.681158Z","shell.execute_reply.started":"2025-02-28T18:11:46.676821Z","shell.execute_reply":"2025-02-28T18:11:46.680228Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"response = requests.post(BASE_URL, json=data, headers=headers)\n\nif response.status_code != 200:\n    print(\"Error:\", response.status_code, response.text)\n    sys.exit()\n    \n\nelaborated_text = response.json()[\"choices\"][0][\"message\"][\"content\"]\nprint(elaborated_text)","metadata":{"_uuid":"98a6cffb-dbb5-4799-8bfd-58a908907b44","_cell_guid":"cf262d81-87b1-48ee-a64b-864a4dd0cb73","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-28T18:11:49.019683Z","iopub.execute_input":"2025-02-28T18:11:49.019980Z","iopub.status.idle":"2025-02-28T18:11:56.759408Z","shell.execute_reply.started":"2025-02-28T18:11:49.019958Z","shell.execute_reply":"2025-02-28T18:11:56.758512Z"}},"outputs":[{"name":"stdout","text":"Beneath a radiant sky, a sea of brilliant blue flowers stretches endlessly, their petals shimmering like polished sapphires in the sunlight. Gentle ripples cascade across the field as a soft breeze whispers through, stirring delicate blossoms into a fluid dance. The air hums with life, rich with the earthy scent of damp soil and sweet floral perfume, while bees dart ceaselessly between blooms. As dusk descends, the blues deepen to indigo, flecked with golden light from the fading sun. A fleeting, tranquil masterpiece.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\nimport numpy as np\n\nprint(\"done\")","metadata":{"_uuid":"611fedb9-0a30-4a26-8a02-5eb23ae502b7","_cell_guid":"406952cf-5d58-4614-8020-29cdf0179e3b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-28T18:12:01.431070Z","iopub.execute_input":"2025-02-28T18:12:01.431503Z","iopub.status.idle":"2025-02-28T18:12:24.121265Z","shell.execute_reply.started":"2025-02-28T18:12:01.431462Z","shell.execute_reply":"2025-02-28T18:12:24.120262Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c0b78490b9a41c5b5f7f6bf042720e4"}},"metadata":{}},{"name":"stdout","text":"done\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torchvision\n\nprint(torch.__version__)\nprint(torch.version.cuda)\nprint(torchvision.__version__)","metadata":{"_uuid":"67ca706a-4f86-48bc-9281-53294a147377","_cell_guid":"3a34a360-5ab0-4573-8998-1c1028c322b2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-28T18:12:41.590390Z","iopub.execute_input":"2025-02-28T18:12:41.590723Z","iopub.status.idle":"2025-02-28T18:12:41.595826Z","shell.execute_reply.started":"2025-02-28T18:12:41.590698Z","shell.execute_reply":"2025-02-28T18:12:41.594863Z"}},"outputs":[{"name":"stdout","text":"2.5.1+cu121\n12.1\n0.20.1+cu121\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = elaborated_text","metadata":{"_uuid":"bf759695-4dd1-4025-8966-ccba24f85517","_cell_guid":"4610235d-d756-4d0d-b9d3-0fbc847289b4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-28T18:12:48.877239Z","iopub.execute_input":"2025-02-28T18:12:48.877587Z","iopub.status.idle":"2025-02-28T18:13:59.071065Z","shell.execute_reply.started":"2025-02-28T18:12:48.877559Z","shell.execute_reply":"2025-02-28T18:13:59.070339Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model_index.json:   0%|          | 0.00/384 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52b7a8ea565a413583edb0092d2faee1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4a9e17ac21e4d90aba245e4e09ce32d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer%2Fvocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b108f799871942dabd0fe58da1314a5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unet%2Fconfig.json:   0%|          | 0.00/787 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1496d10d147a423e8faab287a10e28cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer%2Fspecial_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"379342a308b044199501e13e6ecc7102"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.36G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79ffbc909bc14765a3899073380a059c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer%2Ftokenizer_config.json:   0%|          | 0.00/755 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"febf9a6239094f8c871a322a152dfb7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer%2Fmerges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"414d9bf838214a86b7f34bb7daa79f4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"text_encoder%2Fconfig.json:   0%|          | 0.00/644 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07488febaa654797b290239e35f0c5ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler%2Fscheduler_config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f5e87ade6ce4c4ba6c1aaa19666ddb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vae%2Fconfig.json:   0%|          | 0.00/657 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"107f1e0b13ff44dd898b5abd653b29c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/5.65G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c71dfc8234a44719c67928d104a2b87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59d9033365af4316b4a5543e21d1817f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f5f5f92bd1343d7ad29d68d9862e632"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np  # Import numpy for data type conversion\n\n# Optimize memory usage\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\npipe.unet.enable_forward_chunking(chunk_size=1, dim=1)\npipe.enable_vae_slicing()\n\n#prompt = \"spider-man jumping from a building to another building\"\n\n# Generate video frames first\nvideo_frames = pipe(prompt, negative_prompt=\"low quality\", num_inference_steps=100, num_frames=75).frames\n\n# Convert frames from NumPy to PIL and resize\nvideo_frames = [Image.fromarray((frame * 255).astype(np.uint8)).resize((1024, 576)) for frame in video_frames[0]]\n\n# Export to video\nvideo_path = export_to_video(video_frames)","metadata":{"_uuid":"92054332-634c-4cd4-8332-b3e3a523056c","_cell_guid":"c826fd43-8c5c-4ed2-b3be-82d23faffa39","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-28T18:14:07.511380Z","iopub.execute_input":"2025-02-28T18:14:07.511693Z","iopub.status.idle":"2025-02-28T18:22:31.248977Z","shell.execute_reply.started":"2025-02-28T18:14:07.511669Z","shell.execute_reply":"2025-02-28T18:22:31.247835Z"}},"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (107 > 77). Running this sequence through the model will result in indexing errors\nThe following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. as dusk descends , the blues deepen to indigo , flecked with golden light from the fading sun . a fleeting , tranquil masterpiece .']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abb185f690cf43e59cb92f0cf8ce6717"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import shutil\nshutil.move(video_path, \"/kaggle/working/output_video.mp4\")  # Move to working directory","metadata":{"_uuid":"4e7e79fd-a863-47d1-88df-7596ad00752a","_cell_guid":"2f872827-cf3d-467d-806c-6e5b0cd37b85","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-28T18:22:33.003741Z","iopub.execute_input":"2025-02-28T18:22:33.004035Z","iopub.status.idle":"2025-02-28T18:22:33.029883Z","shell.execute_reply.started":"2025-02-28T18:22:33.004013Z","shell.execute_reply":"2025-02-28T18:22:33.028752Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 18] Invalid cross-device link: '/tmp/tmpjaw9slly.mp4' -> '/kaggle/working/output_video.mp4'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-14e6bdcf9e23>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/kaggle/working/output_video.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move to working directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m             \u001b[0mcopy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/tmpjaw9slly.mp4'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/tmp/tmpjaw9slly.mp4'","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"!pip install open-clip-torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T16:13:11.066422Z","iopub.execute_input":"2025-02-25T16:13:11.066759Z","iopub.status.idle":"2025-02-25T16:13:15.985015Z","shell.execute_reply.started":"2025-02-25T16:13:11.066726Z","shell.execute_reply":"2025-02-25T16:13:15.983977Z"}},"outputs":[{"name":"stdout","text":"Collecting open-clip-torch\n  Downloading open_clip_torch-2.31.0-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (0.20.1+cu121)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (2024.11.6)\nCollecting ftfy (from open-clip-torch)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (4.67.1)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (0.29.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (0.4.5)\nRequirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (1.0.12)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9.0->open-clip-torch) (1.3.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->open-clip-torch) (0.2.13)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (2.32.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->open-clip-torch) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->open-clip-torch) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->open-clip-torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->open-clip-torch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->open-clip-torch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->open-clip-torch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->open-clip-torch) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->open-clip-torch) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->open-clip-torch) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision->open-clip-torch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision->open-clip-torch) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->open-clip-torch) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision->open-clip-torch) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision->open-clip-torch) (2024.2.0)\nDownloading open_clip_torch-2.31.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ftfy, open-clip-torch\nSuccessfully installed ftfy-6.3.1 open-clip-torch-2.31.0\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch\nimport open_clip\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Fix: Only extract model and processor\nmodel = open_clip.create_model(\"ViT-B/32\", pretrained=\"openai\").to(device)\npreprocess = open_clip.image_transform(model.visual.image_size, is_train=False)\ntokenizer = open_clip.get_tokenizer(\"ViT-bigG-14\")\n\ndef compute_clip_score(images, text):\n    \"\"\"Compute CLIP similarity score between images and text\"\"\"\n    text_tokenized = tokenizer([text]).to(device)\n    text_features = model.encode_text(text_tokenized).detach().cpu().numpy()\n\n    image_scores = []\n    for image in images:\n        image_tensor = preprocess(image).unsqueeze(0).to(device)\n        image_features = model.encode_image(image_tensor).detach().cpu().numpy()\n\n        # Compute cosine similarity\n        similarity = np.dot(image_features, text_features.T) / (np.linalg.norm(image_features) * np.linalg.norm(text_features))\n        image_scores.append(similarity)\n\n    return np.mean(image_scores)  # Average CLIP score across frames\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clip_score = compute_clip_score(video_frames, prompt)\nprint(f\"CLIP Score: {clip_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T16:16:41.986505Z","iopub.execute_input":"2025-02-25T16:16:41.986828Z","iopub.status.idle":"2025-02-25T16:16:42.869681Z","shell.execute_reply.started":"2025-02-25T16:16:41.986806Z","shell.execute_reply":"2025-02-25T16:16:42.868737Z"}},"outputs":[{"name":"stdout","text":"CLIP Score: 0.3103\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import CLIPTokenizer, CLIPTextModel\nimport imageio\nimport numpy as np\nimport os\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:06:41.861065Z","iopub.execute_input":"2025-04-19T06:06:41.861415Z","iopub.status.idle":"2025-04-19T06:06:41.865409Z","shell.execute_reply.started":"2025-04-19T06:06:41.861387Z","shell.execute_reply":"2025-04-19T06:06:41.864545Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\ntext_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:06:44.139879Z","iopub.execute_input":"2025-04-19T06:06:44.140197Z","iopub.status.idle":"2025-04-19T06:06:44.645089Z","shell.execute_reply.started":"2025-04-19T06:06:44.140171Z","shell.execute_reply":"2025-04-19T06:06:44.643338Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# **-----------------NEW CODE HERE ONWARDS-----------------**","metadata":{}},{"cell_type":"code","source":"# coding: utf-8\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.init as init\n\n\n# see: _netD in https://github.com/pytorch/examples/blob/master/dcgan/main.py\nclass Discriminator_I(nn.Module):\n    def __init__(self, nc=3, ndf=64, ngpu=1):\n        super(Discriminator_I, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is (nc) x 96 x 96\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 48 x 48\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 24 x 24\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 12 x 12\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 6 x 6\n            nn.Conv2d(ndf * 8, 1, 6, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n        else:\n            output = self.main(input)\n\n        return output.view(-1, 1).squeeze(1)\n\n\nclass Discriminator_V(nn.Module):\n    def __init__(self, nc=3, ndf=64, T=16, ngpu=1):\n        super(Discriminator_V, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is (nc) x T x 96 x 96\n            nn.Conv3d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x T/2 x 48 x 48\n            nn.Conv3d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm3d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x T/4 x 24 x 24\n            nn.Conv3d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm3d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x T/8 x 12 x 12\n            nn.Conv3d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm3d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x T/16  x 6 x 6\n            Flatten(),\n            nn.Linear(int((ndf*8)*(T/16)*6*6), 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n        else:\n            output = self.main(input)\n\n        return output.view(-1, 1).squeeze(1)\n\n\n# see: _netG in https://github.com/pytorch/examples/blob/master/dcgan/main.py\nclass Generator_I(nn.Module):\n    def __init__(self, nc=3, ngf=64, nz=60, ngpu=1):\n        super(Generator_I, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d(     nz, ngf * 8, 6, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            # state size. (ngf*8) x 6 x 6\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            # state size. (ngf*4) x 12 x 12\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # state size. (ngf*2) x 24 x 24\n            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # state size. (ngf) x 48 x 48\n            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 96 x 96\n        )\n\n    def forward(self, input):\n        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n        else:\n            output = self.main(input)\n        return output\n\n\nclass GRU(nn.Module):\n    def __init__(self, input_size, hidden_size, dropout=0, gpu=True):\n        super(GRU, self).__init__()\n\n        output_size      = input_size\n        self._gpu        = gpu\n        self.hidden_size = hidden_size\n\n        # define layers\n        self.gru    = nn.GRUCell(input_size, hidden_size)\n        self.drop   = nn.Dropout(p=dropout)\n        self.linear = nn.Linear(hidden_size, output_size)\n        self.bn     = nn.BatchNorm1d(output_size, affine=False)\n\n    def forward(self, inputs, n_frames):\n        '''\n        inputs.shape()   => (batch_size, input_size)\n        outputs.shape() => (seq_len, batch_size, output_size)\n        '''\n        outputs = []\n        for i in range(n_frames):\n            self.hidden = self.gru(inputs, self.hidden)\n            inputs = self.linear(self.hidden)\n            outputs.append(inputs)\n        outputs = [ self.bn(elm) for elm in outputs ]\n        outputs = torch.stack(outputs)\n        return outputs\n\n    def initWeight(self, init_forget_bias=1):\n        # See details in https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/rnn.py\n        for name, params in self.named_parameters():\n            if 'weight' in name:\n                init.xavier_uniform(params)\n\n            # initialize forget gate bias\n            elif 'gru.bias_ih_l' in name:\n                b_ir, b_iz, b_in = params.chunk(3, 0)\n                init.constant(b_iz, init_forget_bias)\n            elif 'gru.bias_hh_l' in name:\n                b_hr, b_hz, b_hn = params.chunk(3, 0)\n                init.constant(b_hz, init_forget_bias)\n            else:\n                init.constant(params, 0)\n\n    def initHidden(self, batch_size):\n        self.hidden = Variable(torch.zeros(batch_size, self.hidden_size))\n        if self._gpu == True:\n            self.hidden = self.hidden.cuda()\n\n\n''' utils '''\n\nclass Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:06:46.017194Z","iopub.execute_input":"2025-04-19T06:06:46.017575Z","iopub.status.idle":"2025-04-19T06:06:46.040576Z","shell.execute_reply.started":"2025-04-19T06:06:46.017547Z","shell.execute_reply":"2025-04-19T06:06:46.039727Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\nimport os\n\n# Initialize model instances first\nimage_discriminator = Discriminator_I(nc=3, ndf=64)\nvideo_discriminator = Discriminator_V(nc=3, ndf=64, T=16)\ngenerator = Generator_I(nc=3, ngf=64, nz=60)\ngru_model = GRU(input_size=10, hidden_size=100)  # Adjust sizes based on your implementation\n\n# Define model path\nmodel_path = '/kaggle/input/action/pytorch/default/1'  # Adjust as needed\n\n# Load state dictionaries\ndef load_model_state(model, component_name, epoch=110000):\n    model_file = os.path.join(model_path, f\"{component_name}_epoch-{epoch}.model\")\n    \n    # Load the state dictionary\n    state_dict = torch.load(model_file)\n    \n    # If the state_dict is already a state_dict, use it directly\n    if isinstance(state_dict, dict):\n        model.load_state_dict(state_dict)\n    # If the model was saved with torch.save(model) instead of torch.save(model.state_dict())\n    elif hasattr(state_dict, 'state_dict'):\n        model.load_state_dict(state_dict.state_dict())\n    else:\n        # Handle the case where the file might contain something unexpected\n        print(f\"Unexpected format for {component_name}. Type: {type(state_dict)}\")\n    \n    return model\n\n# Load all state dictionaries into models\nimage_discriminator = load_model_state(image_discriminator, 'Discriminator_I')\nvideo_discriminator = load_model_state(video_discriminator, 'Discriminator_V') \ngru_model = load_model_state(gru_model, 'GRU')\ngenerator = load_model_state(generator, 'Generator_I')\n\n# Now you can safely set models to evaluation mode\nimage_discriminator.eval()\nvideo_discriminator.eval()\ngru_model.eval()\ngenerator.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:15:48.125627Z","iopub.execute_input":"2025-04-19T06:15:48.125958Z","iopub.status.idle":"2025-04-19T06:15:48.378887Z","shell.execute_reply.started":"2025-04-19T06:15:48.125931Z","shell.execute_reply":"2025-04-19T06:15:48.377932Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-12-f10ca063012f>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(model_file)\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Generator_I(\n  (main): Sequential(\n    (0): ConvTranspose2d(60, 512, kernel_size=(6, 6), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU(inplace=True)\n    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (11): ReLU(inplace=True)\n    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (13): Tanh()\n  )\n)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport os\nimport imageio\n\n# GPU setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Initialize model instances\nimage_discriminator = Discriminator_I(nc=3, ndf=64).to(device)\nvideo_discriminator = Discriminator_V(nc=3, ndf=64, T=16).to(device)\ngenerator = Generator_I(nc=3, ngf=64, nz=60).to(device)\n\n# For GRU, we need to define dimensions correctly\ndim_z_motion = 10  # Adjust based on your implementation\ngru_model = GRU(input_size=dim_z_motion, hidden_size=100).to(device)\n\n# Load state dictionaries\nmodel_path = '/kaggle/input/action/pytorch/default/1'\n\ndef load_model_state(model, component_name, epoch=110000):\n    model_file = os.path.join(model_path, f\"{component_name}_epoch-{epoch}.model\")\n    state_dict = torch.load(model_file, map_location=device)\n    \n    # If it's already a state_dict\n    if isinstance(state_dict, dict):\n        model.load_state_dict(state_dict)\n    # If it's the full model\n    elif hasattr(state_dict, 'state_dict'):\n        model.load_state_dict(state_dict.state_dict())\n    else:\n        print(f\"Unexpected format for {component_name}\")\n    \n    return model\n\n# Load all models\nimage_discriminator = load_model_state(image_discriminator, 'Discriminator_I')\nvideo_discriminator = load_model_state(video_discriminator, 'Discriminator_V')\ngru_model = load_model_state(gru_model, 'GRU')\ngenerator = load_model_state(generator, 'Generator_I')\n\n# Set models to evaluation mode\nimage_discriminator.eval()\nvideo_discriminator.eval()\ngru_model.eval()\ngenerator.eval()\n\n# Function to generate video\n\n# Generate and save video\ndef save_video(frames, filename='generated_video.mp4', fps=8):\n    imageio.mimsave(filename, frames, fps=fps)\n    return filename\n\n# Generate video\nprint(\"Generating video...\")\nframes = generate_video(num_frames=16)\noutput_file = save_video(frames, '/generated_video.mp4')\nprint(f\"Video saved to {output_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:39:38.789614Z","iopub.execute_input":"2025-04-19T06:39:38.789957Z","iopub.status.idle":"2025-04-19T06:39:39.558157Z","shell.execute_reply.started":"2025-04-19T06:39:38.789928Z","shell.execute_reply":"2025-04-19T06:39:39.557049Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-20-af979ab98a17>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(model_file, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Generating video...\nVideo saved to /generated_video.mp4\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import torch\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n# 1. Setup text encoder\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# 2. Function to convert text prompt to latent vector\ndef prompt_to_latent(prompt, dim_z_content=50, dim_z_motion=10):\n    # Encode text\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n    with torch.no_grad():\n        text_embeddings = text_encoder(**inputs).last_hidden_state.mean(dim=1)\n    \n    # Project CLIP embedding to content vector (50D)\n    content_proj = torch.nn.Linear(text_embeddings.shape[1], dim_z_content)\n    z_content = content_proj(text_embeddings)\n    \n    # Use a fixed category or derive from prompt aspects\n    z_category = torch.zeros(1, 6)  # Default category vector (adjust size as needed)\n    \n    # Motion is typically dynamic and generated by GRU\n    # Here we initialize it randomly\n    z_motion = torch.randn(1, dim_z_motion)\n    \n    # Combine all vectors - TOTAL MUST BE 60 for your model\n    z_combined = torch.cat([z_content, z_category, z_motion], dim=1)\n    \n    # Make sure combined vector is exactly 60 dimensions\n    assert z_combined.shape[1] == 60, \"Combined vector must be 60 dimensions\"\n    \n    # Reshape for generator\n    z_combined = z_combined.view(1, 60, 1, 1)\n    \n    return z_combined, z_motion\n\n# 3. Generate video from prompt\ndef generate_video(num_frames=16, dim_z_content=50, dim_z_category=0, dim_z_motion=10):\n    # Create random content vector (this would be mapped from your prompt in a text-guided system)\n    z_content = torch.randn(1, dim_z_content).to(device)\n    z_category = torch.zeros(1, dim_z_category).to(device)  # Can be used for category conditioning\n    \n    # Initialize GRU\n    gru_model.initHidden(batch_size=1)\n    \n    # Initial motion input\n    z_motion_input = torch.randn(1, dim_z_motion).to(device)\n    \n    # Generate frames\n    frames = []\n    with torch.no_grad():\n        for i in range(num_frames):\n            # Get motion vector from GRU\n            if i == 0:\n                motion = z_motion_input\n            else:\n                # Update motion using GRU\n                motion = gru_model.gru(z_motion_input, gru_model.hidden)\n                gru_model.hidden = motion\n                motion = gru_model.linear(motion)\n            \n            # Concatenate content, category and motion vectors\n            z = torch.cat([z_content, z_category, motion], dim=1)\n            \n            # Ensure exactly 60 dimensions for generator\n            if z.size(1) > 60:\n                z = z[:, :60]\n            elif z.size(1) < 60:\n                padding = torch.zeros(z.size(0), 60-z.size(1)).to(device)\n                z = torch.cat([z, padding], dim=1)\n                \n            # Reshape for generator\n            z = z.view(z.size(0), z.size(1), 1, 1)\n            \n            # Generate frame\n            fake = generator(z)\n            \n            # Convert to image\n            frame = fake.squeeze().cpu().numpy().transpose(1, 2, 0)\n            frame = ((frame + 1) / 2 * 255).astype(np.uint8)  # Convert from [-1,1] to [0,255]\n            frames.append(frame)\n    \n    return frames\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:39:57.238787Z","iopub.execute_input":"2025-04-19T06:39:57.239143Z","iopub.status.idle":"2025-04-19T06:40:01.981509Z","shell.execute_reply.started":"2025-04-19T06:39:57.239117Z","shell.execute_reply":"2025-04-19T06:40:01.980208Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Update this line to match your actual path\nmodel_path = '/kaggle/input/action/pytorch/default/1'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:42:03.752922Z","iopub.execute_input":"2025-04-19T06:42:03.753299Z","iopub.status.idle":"2025-04-19T06:42:03.757633Z","shell.execute_reply.started":"2025-04-19T06:42:03.753272Z","shell.execute_reply":"2025-04-19T06:42:03.756648Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def generate_video_from_prompt(prompt, num_frames=16):\n    print(f\"Generating video for prompt: {prompt}\")\n    \n    # Move text encoder to the same device as your models\n    text_encoder.to(device)\n    \n    # Get latent vectors from the prompt\n    with torch.no_grad():\n        # Encode text\n        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n        text_embeddings = text_encoder(**inputs).last_hidden_state.mean(dim=1)\n        \n        # Project CLIP embedding to content vector (50D)\n        content_proj = torch.nn.Linear(text_embeddings.shape[1], 50).to(device)\n        z_content = content_proj(text_embeddings)\n        \n        # Use a fixed category vector (optional conditioning)\n        z_category = torch.zeros(1, 0).to(device)  # Zero dimensions for category\n        \n        # Initialize GRU\n        gru_model.initHidden(batch_size=1)\n        \n        # Initial motion input\n        z_motion_input = torch.randn(1, 10).to(device)\n        \n        # Generate frames\n        frames = []\n        for i in range(num_frames):\n            # Get motion vector from GRU\n            if i == 0:\n                motion = z_motion_input\n            else:\n                # Update motion using GRU\n                motion = gru_model.gru(z_motion_input, gru_model.hidden)\n                gru_model.hidden = motion\n                motion = gru_model.linear(motion)\n            \n            # Concatenate content, category and motion vectors\n            z = torch.cat([z_content, z_category, motion], dim=1)\n            \n            # Ensure exactly 60 dimensions for generator\n            if z.size(1) > 60:\n                z = z[:, :60]\n            elif z.size(1) < 60:\n                padding = torch.zeros(z.size(0), 60-z.size(1)).to(device)\n                z = torch.cat([z, padding], dim=1)\n                \n            # Reshape for generator\n            z = z.view(z.size(0), z.size(1), 1, 1)\n            \n            # Generate frame\n            fake = generator(z)\n            \n            # Convert to image\n            frame = fake.squeeze().cpu().numpy().transpose(1, 2, 0)\n            frame = ((frame + 1) / 2 * 255).astype(np.uint8)  # Convert from [-1,1] to [0,255]\n            frames.append(frame)\n        \n        return frames\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:42:11.822798Z","iopub.execute_input":"2025-04-19T06:42:11.823168Z","iopub.status.idle":"2025-04-19T06:42:11.832003Z","shell.execute_reply.started":"2025-04-19T06:42:11.823141Z","shell.execute_reply":"2025-04-19T06:42:11.830779Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Install required packages if not already installed\n!pip install transformers imageio imageio-ffmpeg\n\n# Example prompts to try\nexample_prompts = [\n    \"A person dancing\",\n    \"A face showing surprise\",\n    \"A person doing backflips\",\n    \"A cat playing\",\n    \"A hand waving\"\n]\n\n# Choose a prompt or create your own\nprompt = \"A face showing happiness\"  # Replace with your desired prompt\n\n# Generate the video\nvideo_frames = generate_video_from_prompt(prompt, num_frames=16)\n\n# Save the generated video\noutput_path = '/kaggle/working/'  # Kaggle's writable directory\noutput_file = save_video(video_frames, f\"{output_path}prompt_{prompt.replace(' ', '_')[:20]}.mp4\")\nprint(f\"Video saved to {output_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:42:20.678201Z","iopub.execute_input":"2025-04-19T06:42:20.678496Z","iopub.status.idle":"2025-04-19T06:42:26.945840Z","shell.execute_reply.started":"2025-04-19T06:42:20.678473Z","shell.execute_reply":"2025-04-19T06:42:26.944570Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.36.1)\nRequirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (0.5.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (11.0.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg) (75.1.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nGenerating video for prompt: A face showing happiness\nVideo saved to /kaggle/working/prompt_A_face_showing_happi.mp4\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"def generate_variations(prompt, num_variations=3):\n    for i in range(num_variations):\n        # Set a different seed for each variation\n        torch.manual_seed(i)\n        \n        # Generate video\n        print(f\"Generating variation {i+1} for prompt: {prompt}\")\n        frames = generate_video_from_prompt(prompt)\n        \n        # Save video\n        output_file = save_video(frames, f\"/kaggle/working/prompt_{prompt.replace(' ', '_')[:20]}_variation_{i+1}.mp4\")\n        print(f\"Variation {i+1} saved to {output_file}\")\n\n# Generate multiple variations\ngenerate_variations(\"A person dancing\", num_variations=3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:42:32.973109Z","iopub.execute_input":"2025-04-19T06:42:32.973476Z","iopub.status.idle":"2025-04-19T06:42:33.277308Z","shell.execute_reply.started":"2025-04-19T06:42:32.973444Z","shell.execute_reply":"2025-04-19T06:42:33.276031Z"}},"outputs":[{"name":"stdout","text":"Generating variation 1 for prompt: A person dancing\nGenerating video for prompt: A person dancing\nVariation 1 saved to /kaggle/working/prompt_A_person_dancing_variation_1.mp4\nGenerating variation 2 for prompt: A person dancing\nGenerating video for prompt: A person dancing\nVariation 2 saved to /kaggle/working/prompt_A_person_dancing_variation_2.mp4\nGenerating variation 3 for prompt: A person dancing\nGenerating video for prompt: A person dancing\nVariation 3 saved to /kaggle/working/prompt_A_person_dancing_variation_3.mp4\n","output_type":"stream"}],"execution_count":25}]}